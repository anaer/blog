<div class="markdown-heading"><h2 class="heading-element">ollama安装</h2><a id="user-content-ollama安装" class="anchor" aria-label="Permalink: ollama安装" href="#ollama安装"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>可以按照官方的安装手册进行安装, 这里仅记录自己的操作步骤方便记忆.
<a href="https://github.com/ollama/ollama/blob/main/docs/linux.md">Linux安装ollama手册</a></p>
<div class="highlight highlight-source-shell"><pre>curl -fsSL https://ollama.com/install.sh <span class="pl-k">|</span> sudo bash</pre></div>
<p>使用N卡的话, 建议使用安装脚本安装, 会安装相关驱动并配置.
如官网下载速度太慢, 可自己下载包并修改脚本</p>
<div class="highlight highlight-source-diff"><pre><span class="pl-mdr">82c82,84</span>
<span class="pl-md"><span class="pl-md">&lt;</span> $SUDO tar -C "$OLLAMA_INSTALL_DIR" -xzf ollama-linux-amd64.tgz</span>
<span class="pl-ms">---</span>
<span class="pl-mi1"><span class="pl-mi1">&gt;</span> curl --fail --show-error --location --progress-bar \</span>
<span class="pl-mi1"><span class="pl-mi1">&gt;</span>     "https://ollama.com/download/ollama-linux-${ARCH}.tgz${VER_PARAM}" | \</span>
<span class="pl-mi1"><span class="pl-mi1">&gt;</span>     $SUDO tar -xzf - -C "$OLLAMA_INSTALL_DIR"</span></pre></div>
<p>PS: 以Ubuntu 14.04.5 LTS为例</p>
<div class="markdown-heading"><h3 class="heading-element">下载安装<a href="https://github.com/ollama/ollama/releases/tag/v0.5.7">ollama v0.5.7</a>
</h3><a id="user-content-下载安装ollama-v057" class="anchor" aria-label="Permalink: 下载安装ollama v0.5.7" href="#下载安装ollama-v057"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>将压缩包解压到/usr目录, 可以不用再配置环境变量直接调用</p>
<div class="highlight highlight-source-shell"><pre>sudo tar -C /usr -xzf ollama-linux-amd64.tgz</pre></div>
<p>查询压缩包的结构</p>
<pre lang="log"><code># tar -tvf ollama-linux-amd64.tgz
drwxr-xr-x root/root         0 2025-01-17 00:47 ./
drwxr-xr-x root/root         0 2025-01-17 00:47 ./bin/
-rwxr-xr-x root/root  30064856 2025-01-17 00:47 ./bin/ollama
drwxr-xr-x root/root         0 2025-01-17 00:47 ./lib/
drwxr-xr-x root/root         0 2025-01-17 00:52 ./lib/ollama/
lrwxrwxrwx root/root         0 2025-01-17 00:52 ./lib/ollama/libcudart.so.12 -&gt; libcudart.so.12.4.127
-rwxr-xr-x root/root 441938896 2025-01-17 00:52 ./lib/ollama/libcublasLt.so.12.4.5.8
lrwxrwxrwx root/root         0 2025-01-17 00:49 ./lib/ollama/libcudart.so.11.0 -&gt; libcudart.so.11.3.109
lrwxrwxrwx root/root         0 2025-01-17 00:49 ./lib/ollama/libcublas.so.11 -&gt; libcublas.so.11.5.1.109
lrwxrwxrwx root/root         0 2025-01-17 00:52 ./lib/ollama/libcublas.so.12 -&gt; ./libcublas.so.12.4.5.8
-rwxr-xr-x root/root    619192 2025-01-17 00:52 ./lib/ollama/libcudart.so.11.3.109
-rwxr-xr-x root/root 109604768 2025-01-17 00:52 ./lib/ollama/libcublas.so.12.4.5.8
-rwxr-xr-x root/root    707904 2025-01-17 00:52 ./lib/ollama/libcudart.so.12.4.127
-rwxr-xr-x root/root 263770264 2025-01-17 00:49 ./lib/ollama/libcublasLt.so.11.5.1.109
lrwxrwxrwx root/root         0 2025-01-17 00:49 ./lib/ollama/libcublasLt.so.11 -&gt; libcublasLt.so.11.5.1.109
drwxr-xr-x root/root         0 2025-01-17 01:15 ./lib/ollama/runners/
drwxr-xr-x root/root         0 2025-01-17 01:03 ./lib/ollama/runners/cuda_v11_avx/
-rwxr-xr-x root/root   9885296 2025-01-17 01:03 ./lib/ollama/runners/cuda_v11_avx/ollama_llama_server
-rwxr-xr-x root/root 979085896 2025-01-17 01:01 ./lib/ollama/runners/cuda_v11_avx/libggml_cuda_v11.so
drwxr-xr-x root/root         0 2025-01-17 01:17 ./lib/ollama/runners/rocm_avx/
-rwxr-xr-x root/root   9930480 2025-01-17 01:17 ./lib/ollama/runners/rocm_avx/ollama_llama_server
-rwxr-xr-x root/root 451342832 2025-01-17 01:15 ./lib/ollama/runners/rocm_avx/libggml_rocm.so
drwxr-xr-x root/root         0 2025-01-17 01:01 ./lib/ollama/runners/cuda_v12_avx/
-rwxr-xr-x root/root   9873136 2025-01-17 01:01 ./lib/ollama/runners/cuda_v12_avx/ollama_llama_server
-rwxr-xr-x root/root 1237676328 2025-01-17 00:59 ./lib/ollama/runners/cuda_v12_avx/libggml_cuda_v12.so
drwxr-xr-x root/root          0 2025-01-17 00:47 ./lib/ollama/runners/cpu_avx2/
-rwxr-xr-x root/root    9860720 2025-01-17 00:47 ./lib/ollama/runners/cpu_avx2/ollama_llama_server
drwxr-xr-x root/root          0 2025-01-17 00:47 ./lib/ollama/runners/cpu_avx/
-rwxr-xr-x root/root    9840240 2025-01-17 00:47 ./lib/ollama/runners/cpu_avx/ollama_llama_server
-rwxr-xr-x root/root  121866104 2025-01-17 00:49 ./lib/ollama/libcublas.so.11.5.1.109
lrwxrwxrwx root/root          0 2025-01-17 00:52 ./lib/ollama/libcublasLt.so.12 -&gt; ./libcublasLt.so.12.4.5.8
</code></pre>
<div class="markdown-heading"><h3 class="heading-element">启动ollama</h3><a id="user-content-启动ollama" class="anchor" aria-label="Permalink: 启动ollama" href="#启动ollama"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>长期使用可以按照安装手册, 添加ollama到系统服务中.</p>
<div class="markdown-heading"><h4 class="heading-element">系统服务</h4><a id="user-content-系统服务" class="anchor" aria-label="Permalink: 系统服务" href="#系统服务"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<pre lang="conf"><code>[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/local/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
Environment="PATH=/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin"
Environment="OLLAMA_HOST=0.0.0.0"
Environment="OLLAMA_MODELS=/data/ollama/models"
Environment="OLLAMA_ORIGINS=*"
Environment="OLLAMA_DEBUG=1"

[Install]
WantedBy=default.target
</code></pre>
<p>OLLAMA_HOST设置为0.0.0.0 允许局域网访问</p>
<p>OLLAMA_MODELS 指定模型路径 防止系统盘空间不足</p>
<p>OLLAMA_ORIGINS设置跨域, 可在<a href="https://github.com/josStorer/chatGPTBox">chatgptBox</a>中使用</p>
<p>OLLAMA_DEBUG 设置debug模式, 可查看用户请求的参数信息</p>
<div class="markdown-heading"><h4 class="heading-element">手动启动</h4><a id="user-content-手动启动" class="anchor" aria-label="Permalink: 手动启动" href="#手动启动"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<div class="highlight highlight-source-shell"><pre>nohup /usr/bin/ollama serve <span class="pl-k">&amp;</span></pre></div>
<div class="markdown-heading"><h3 class="heading-element">命令执行</h3><a id="user-content-命令执行" class="anchor" aria-label="Permalink: 命令执行" href="#命令执行"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>查询本地已下载的模型</strong></p>
<div class="highlight highlight-source-shell"><pre>$ ollama list
NAME                ID              SIZE      MODIFIED
deepseek-r1:1.5b    a42b25d8c10a    1.1 GB    24 hours ago</pre></div>
<p><strong>拉取模型</strong></p>
<div class="highlight highlight-source-shell"><pre>ollama pull deepseek-r1:1.5b</pre></div>
<p><strong>启动模型</strong>
直接在命令行进行交互</p>
<div class="highlight highlight-source-shell"><pre>$ ollama run deepseek-r1:1.5b
&gt;&gt;&gt; 你是什么
<span class="pl-k">&lt;</span>think<span class="pl-k">&gt;</span>

<span class="pl-k">&lt;</span>/think<span class="pl-k">&gt;</span>

您好！我是由中国的深度求索（DeepSeek）公司开发的智能助手DeepSeek-R1。如您有任何任何问题，我会尽我所能为您提供帮助。</pre></div>
<div class="markdown-heading"><h3 class="heading-element">测试接口</h3><a id="user-content-测试接口" class="anchor" aria-label="Permalink: 测试接口" href="#测试接口"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<div class="highlight highlight-source-batchfile"><pre>curl -L -X POST 'http://127.0.0.1:11434/v1/chat/completions' \
-H 'Content-Type: application/json' \
-H 'Authorization: Bearer xxx' \
-H 'Accept: application/json' \
-d '{
  <span class="pl-s"><span class="pl-pds">"</span>messages<span class="pl-pds">"</span></span>: [
    {
      <span class="pl-s"><span class="pl-pds">"</span>content<span class="pl-pds">"</span></span>: <span class="pl-s"><span class="pl-pds">"</span>You are a helpful assistant<span class="pl-pds">"</span></span>,
      <span class="pl-s"><span class="pl-pds">"</span>role<span class="pl-pds">"</span></span>: <span class="pl-s"><span class="pl-pds">"</span>system<span class="pl-pds">"</span></span>
    },
    {
      <span class="pl-s"><span class="pl-pds">"</span>content<span class="pl-pds">"</span></span>: <span class="pl-s"><span class="pl-pds">"</span>Hi<span class="pl-pds">"</span></span>,
      <span class="pl-s"><span class="pl-pds">"</span>role<span class="pl-pds">"</span></span>: <span class="pl-s"><span class="pl-pds">"</span>user<span class="pl-pds">"</span></span>
    }
  ],
  <span class="pl-s"><span class="pl-pds">"</span>model<span class="pl-pds">"</span></span>: <span class="pl-s"><span class="pl-pds">"</span>deepseek-r1:1.5b<span class="pl-pds">"</span></span>,
  <span class="pl-s"><span class="pl-pds">"</span>frequency_penalty<span class="pl-pds">"</span></span>: 0,
  <span class="pl-s"><span class="pl-pds">"</span>max_tokens<span class="pl-pds">"</span></span>: 2048,
  <span class="pl-s"><span class="pl-pds">"</span>presence_penalty<span class="pl-pds">"</span></span>: 0,
  <span class="pl-s"><span class="pl-pds">"</span>response_format<span class="pl-pds">"</span></span>: {
    <span class="pl-s"><span class="pl-pds">"</span>type<span class="pl-pds">"</span></span>: <span class="pl-s"><span class="pl-pds">"</span>text<span class="pl-pds">"</span></span>
  },
  <span class="pl-s"><span class="pl-pds">"</span>stop<span class="pl-pds">"</span></span>: null,
  <span class="pl-s"><span class="pl-pds">"</span>stream<span class="pl-pds">"</span></span>: false,
  <span class="pl-s"><span class="pl-pds">"</span>stream_options<span class="pl-pds">"</span></span>: null,
  <span class="pl-s"><span class="pl-pds">"</span>temperature<span class="pl-pds">"</span></span>: 1,
  <span class="pl-s"><span class="pl-pds">"</span>top_p<span class="pl-pds">"</span></span>: 1,
  <span class="pl-s"><span class="pl-pds">"</span>tools<span class="pl-pds">"</span></span>: null,
  <span class="pl-s"><span class="pl-pds">"</span>tool_choice<span class="pl-pds">"</span></span>: <span class="pl-s"><span class="pl-pds">"</span>none<span class="pl-pds">"</span></span>,
  <span class="pl-s"><span class="pl-pds">"</span>logprobs<span class="pl-pds">"</span></span>: false,
  <span class="pl-s"><span class="pl-pds">"</span>top_logprobs<span class="pl-pds">"</span></span>: null
}'</pre></div>
<div class="markdown-heading"><h3 class="heading-element">nginx配置</h3><a id="user-content-nginx配置" class="anchor" aria-label="Permalink: nginx配置" href="#nginx配置"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<pre><code>location /ollama/ {
    proxy_pass http://127.0.0.1:11434/;
    proxy_http_version 1.1;
    proxy_set_header Upgrade $http_upgrade;
    proxy_set_header Connection 'upgrade';
    proxy_set_header Host $host;
    proxy_cache_bypass $http_upgrade;
}
</code></pre>
<div class="markdown-heading"><h2 class="heading-element">本地客户端 <a href="https://github.com/CherryHQ/cherry-studio/releases/tag/v0.9.21">Cherry Studio</a>
</h2><a id="user-content-本地客户端-cherry-studio" class="anchor" aria-label="Permalink: 本地客户端 Cherry Studio" href="#本地客户端-cherry-studio"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>设置-&gt;模型服务-&gt;Ollama</p>
<p>API密钥: 空</p>
<p>API地址: <a href="http://mydomain.com/ollama/v1/" rel="nofollow">http://mydomain.com/ollama/v1/</a></p>
<p>添加模型: deepseek-r1:1.5b</p>
<p>在助手页设置选择默认模型为上述模型 即可进行聊天</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/user-attachments/assets/2127d6b0-7229-44d0-bac7-4b31557e325e"><img src="https://github.com/user-attachments/assets/2127d6b0-7229-44d0-bac7-4b31557e325e" alt="Image" style="max-width: 100%;"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/user-attachments/assets/f4f0d98d-b353-4367-a7c5-f02c2a2eda0e"><img src="https://github.com/user-attachments/assets/f4f0d98d-b353-4367-a7c5-f02c2a2eda0e" alt="Image" style="max-width: 100%;"></a></p>
<div class="markdown-heading"><h2 class="heading-element">anythingllm</h2><a id="user-content-anythingllm" class="anchor" aria-label="Permalink: anythingllm" href="#anythingllm"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>AnythingLLM 是一个功能强大且灵活的开源平台，旨在帮助用户轻松构建和部署基于大型语言模型 (LLM) 的私有化应用程序。</p>
<p>RAG（Retrieval-Augmented Generation）增强信息检索和生成模型，能够从大规模知识库中检索相关信息并生成高质量的反馈。
简单来讲，采用RAG就需要把你的私域数据向量化，然后存储到向量数据库中，支持向量检索配合LLM大模型一起提供更专业的回复。
RAG是一种结合了信息检索和大模型（LLM）的技术，在对抗大模型幻觉、高效管理用户本地文件以及数据安全保护等方面具有独到的优势。</p>
<p>要求: 服务器安装docker</p>
<div class="highlight highlight-source-shell"><pre><span class="pl-s"><span class="pl-pds">"</span>registry-mirrors<span class="pl-pds">"</span></span>: [
    <span class="pl-s"><span class="pl-pds">"</span>https://docker.hpcloud.cloud/<span class="pl-pds">"</span></span>,
    <span class="pl-s"><span class="pl-pds">"</span>https://docker.m.daocloud.io/<span class="pl-pds">"</span></span>,
    <span class="pl-s"><span class="pl-pds">"</span>https://docker.unsee.tech/<span class="pl-pds">"</span></span>,
    <span class="pl-s"><span class="pl-pds">"</span>https://docker.1panel.live/<span class="pl-pds">"</span></span>,
    <span class="pl-s"><span class="pl-pds">"</span>http://mirrors.ustc.edu.cn/<span class="pl-pds">"</span></span>,
    <span class="pl-s"><span class="pl-pds">"</span>https://docker.chenby.cn/<span class="pl-pds">"</span></span>,
    <span class="pl-s"><span class="pl-pds">"</span>http://mirror.azure.cn/<span class="pl-pds">"</span></span>,
    <span class="pl-s"><span class="pl-pds">"</span>https://dockerpull.org/<span class="pl-pds">"</span></span>,
    <span class="pl-s"><span class="pl-pds">"</span>https://dockerhub.icu/<span class="pl-pds">"</span></span>,
    <span class="pl-s"><span class="pl-pds">"</span>https://hub.rat.dev/<span class="pl-pds">"</span></span>
  ]</pre></div>
<p>install.sh 安装脚本</p>
<div class="highlight highlight-source-shell"><pre><span class="pl-k">export</span> STORAGE_LOCATION=<span class="pl-smi">$HOME</span>/anythingllm <span class="pl-k">&amp;&amp;</span> \
mkdir -p <span class="pl-smi">$STORAGE_LOCATION</span> <span class="pl-k">&amp;&amp;</span> \
touch <span class="pl-s"><span class="pl-pds">"</span><span class="pl-smi">$STORAGE_LOCATION</span>/.env<span class="pl-pds">"</span></span> <span class="pl-k">&amp;&amp;</span> \
docker run -d -p 3001:3001 \
--cap-add SYS_ADMIN \
-v <span class="pl-smi">${STORAGE_LOCATION}</span>:/app/server/storage \
-v <span class="pl-smi">${STORAGE_LOCATION}</span>/.env:/app/server/.env \
-e STORAGE_DIR=<span class="pl-s"><span class="pl-pds">"</span>/app/server/storage<span class="pl-pds">"</span></span> \
mintplexlabs/anythingllm</pre></div>
<p>执行该脚本会在当前用户目录下anythingllm目录安装服务</p>
<div class="markdown-heading"><h3 class="heading-element">nginx配置</h3><a id="user-content-nginx配置-1" class="anchor" aria-label="Permalink: nginx配置" href="#nginx配置-1"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<pre><code>location / {
    proxy_pass http://127.0.0.1:3001;
}
</code></pre>
<div class="markdown-heading"><h3 class="heading-element">配置AnythingLLM</h3><a id="user-content-配置anythingllm" class="anchor" aria-label="Permalink: 配置AnythingLLM" href="#配置anythingllm"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>嵌入模型
AnythingLLM Embedder(默认)
nomic-embed-text</p>
<div class="highlight highlight-source-shell"><pre>ollama pull nomic-embed-text</pre></div>
<p>接口文档: <a href="http://127.0.0.1:3001/api/docs" rel="nofollow">http://127.0.0.1:3001/api/docs</a></p>
<p>首次访问时按导引进行模型配置, 配置URL后会自动获取支持的模型列表</p>
<p>人工智能提供商 -&gt; LLM首选项</p>
<p>LLM提供商: Ollama</p>
<p>Ollama Model: deepseek-r1:1.5b</p>
<p>Ollama Base URL: <a href="http://mydomain.com/ollama" rel="nofollow">http://mydomain.com/ollama</a></p>
<div class="markdown-heading"><h3 class="heading-element">新工作区</h3><a id="user-content-新工作区" class="anchor" aria-label="Permalink: 新工作区" href="#新工作区"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>创建工作区后, 可通过上传图标按钮, 上传自有文档添加到工作区中
也可以连接GitHub Repo, GitLab Repo, YouTube Transcript, Bulk Link Scraper, Confluence</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/user-attachments/assets/0d4acaa7-ed97-4576-84f4-25099e85abae"><img src="https://github.com/user-attachments/assets/0d4acaa7-ed97-4576-84f4-25099e85abae" alt="Image" style="max-width: 100%;"></a></p>
<div class="markdown-heading"><h3 class="heading-element">嵌入式对话</h3><a id="user-content-嵌入式对话" class="anchor" aria-label="Permalink: 嵌入式对话" href="#嵌入式对话"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>可新增嵌入式对话, 按需配置后(需选择工作区), Show Code复制代码, 将代码复制到空白html的</p>内, 可直接打开html测试
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/user-attachments/assets/3d7f9e47-9d59-4e7e-adfe-4c84b00c08e2"><img src="https://github.com/user-attachments/assets/3d7f9e47-9d59-4e7e-adfe-4c84b00c08e2" alt="Image" style="max-width: 100%;"></a></p>
<div class="markdown-heading"><h2 class="heading-element">监控gpu使用情况</h2><a id="user-content-监控gpu使用情况" class="anchor" aria-label="Permalink: 监控gpu使用情况" href="#监控gpu使用情况"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<div class="highlight highlight-source-shell"><pre>watch -n 1 nvidia-smi</pre></div>
<div class="markdown-heading"><h2 class="heading-element">相关链接</h2><a id="user-content-相关链接" class="anchor" aria-label="Permalink: 相关链接" href="#相关链接"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><a href="https://github.com/ollama/ollama">ollama</a></p>
<p><a href="https://github.com/ollama/ollama/tree/main?tab=readme-ov-file#model-library">ollama常用模型</a></p>
<p><a href="https://ollama.com/library" rel="nofollow">ollama模型列表</a></p>
<p><a href="https://cloud.tencent.com/developer/special/deepseek" rel="nofollow">DeepSeek技术专题：部署教程、一线玩法、原理解析</a></p>
<p><a href="https://cloud.tencent.com/document/product/851/115962?from=25520" rel="nofollow">TI-ONE 训练平台 快速部署和体验 DeepSeek 系列模型</a></p>
<p><a href="https://api-docs.deepseek.com/zh-cn/prompt-library" rel="nofollow">Deepseek官方提示词库</a></p>
<p><a href="https://ragflow.io/" rel="nofollow">RAGFlow</a></p>
<p><a href="https://github.com/weekend-project-space/free-ollama">Ollama免费服务</a></p>
